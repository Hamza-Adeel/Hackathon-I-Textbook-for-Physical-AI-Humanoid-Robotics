{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"roboticsTextbookSidebar":[{"type":"category","label":"Chapter 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/docs/ros-2/understanding-nodes-topics-services","label":"Nodes, Topics, Services","docId":"ros-2/understanding-nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/ros-2/python-rclpy","label":"Python Node (RCLPy)","docId":"ros-2/python-rclpy","unlisted":false},{"type":"link","href":"/docs/ros-2/urdf-for-humanoids","label":"URDF for Humanoids","docId":"ros-2/urdf-for-humanoids","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Chapter 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/docs/digital-twin/setup-environments","label":"Setup Environments","docId":"digital-twin/setup-environments","unlisted":false},{"type":"link","href":"/docs/digital-twin/physics-simulation","label":"Physics Simulation","docId":"digital-twin/physics-simulation","unlisted":false},{"type":"link","href":"/docs/digital-twin/simulating-sensors","label":"Simulating Sensors","docId":"digital-twin/simulating-sensors","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Chapter 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/docs/nvidia-isaac/intro-to-isaac-sim","label":"Intro to Isaac Sim","docId":"nvidia-isaac/intro-to-isaac-sim","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac/synthetic-data","label":"Synthetic Data","docId":"nvidia-isaac/synthetic-data","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac/nav2-path-planning","label":"Nav2 Path Planning","docId":"nvidia-isaac/nav2-path-planning","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Chapter 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/docs/vla/whisper-voice-commands","label":"Whisper Voice Commands","docId":"vla/whisper-voice-commands","unlisted":false},{"type":"link","href":"/docs/vla/llm-cognitive-planning","label":"Cognitive Planning with LLMs","docId":"vla/llm-cognitive-planning","unlisted":false},{"type":"link","href":"/docs/vla/capstone-project","label":"Capstone Project","docId":"vla/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"digital-twin/physics-simulation":{"id":"digital-twin/physics-simulation","title":"The Physics of Simulation","description":"A digital twin is more than just a 3D model; it's a dynamic representation that behaves like its real-world counterpart. The magic behind this realism is the physics engine, a core component of any robotics simulator.","sidebar":"roboticsTextbookSidebar"},"digital-twin/setup-environments":{"id":"digital-twin/setup-environments","title":"Setting up Simulation Environments","description":"Welcome to the second chapter, where we dive into the world of Digital Twins. A digital twin is a virtual model of a physical object or system, and in robotics, simulation is our primary tool for creating and interacting with them.","sidebar":"roboticsTextbookSidebar"},"digital-twin/simulating-sensors":{"id":"digital-twin/simulating-sensors","title":"Simulating Complex Sensors","description":"For a digital twin to be useful, it must not only simulate the robot's motion but also the data it perceives from its environment. Modern robotics simulators provide a wide array of plugins for modeling complex sensors.","sidebar":"roboticsTextbookSidebar"},"nvidia-isaac/intro-to-isaac-sim":{"id":"nvidia-isaac/intro-to-isaac-sim","title":"Introduction to Isaac Sim","description":"Welcome to Chapter 3, where we explore NVIDIA's powerful robotics platform, Isaac Sim. While Gazebo is a fantastic general-purpose robotics simulator, Isaac Sim excels in creating high-fidelity, photorealistic worlds, making it a premier choice for developing AI-powered robots that rely on vision and perception.","sidebar":"roboticsTextbookSidebar"},"nvidia-isaac/isaac-ros-vslam":{"id":"nvidia-isaac/isaac-ros-vslam","title":"Implementing Isaac ROS VSLAM & Navigation","description":"This lesson covers implementing Visual SLAM (VSLAM) and navigation capabilities using NVIDIA Isaac ROS modules within Isaac Sim."},"nvidia-isaac/nav2-path-planning":{"id":"nvidia-isaac/nav2-path-planning","title":"Integrating Nav2 for Path Planning","description":"While Isaac Sim is a powerful tool for perception and AI training, robotics is fundamentally about moving and acting in an environment. To achieve autonomous navigation, we can integrate our Isaac Sim simulation with Nav2, the standard navigation stack in ROS 2.","sidebar":"roboticsTextbookSidebar"},"nvidia-isaac/synthetic-data":{"id":"nvidia-isaac/synthetic-data","title":"Generating Synthetic Data for Training","description":"One of the most significant bottlenecks in developing modern, AI-powered robots is the need for vast amounts of high-quality, labeled training data. Manually collecting and labeling thousands of images or sensor readings is slow, expensive, and error-prone.","sidebar":"roboticsTextbookSidebar"},"ros-2/python-rclpy":{"id":"ros-2/python-rclpy","title":"Writing a Python ROS 2 Node (RCLPy)","description":"In our previous lesson, we learned about the theoretical concepts of nodes, topics, and services. Now, it's time to put that theory into practice by writing our first ROS 2 node using rclpy, the official Python client library for ROS 2.","sidebar":"roboticsTextbookSidebar"},"ros-2/understanding-nodes-topics-services":{"id":"ros-2/understanding-nodes-topics-services","title":"Understanding Nodes, Topics, and Services","description":"Welcome to the first lesson in our study of the Robotic Nervous System (ROS 2). In this section, we will explore the three fundamental concepts that form the backbone of all ROS 2 applications: Nodes, Topics, and Services.","sidebar":"roboticsTextbookSidebar"},"ros-2/urdf-for-humanoids":{"id":"ros-2/urdf-for-humanoids","title":"Building URDF for Humanoid Models","description":"A crucial part of robotics simulation and visualization is having an accurate model of the robot itself. In the ROS ecosystem, the standard for describing a robot's physical structure is the Unified Robot Description Format (URDF).","sidebar":"roboticsTextbookSidebar"},"vla/capstone-project":{"id":"vla/capstone-project","title":"Capstone Project - VLA-Powered Task Execution","description":"In this final lesson, we will bring together all the concepts from this textbook to create a complete Vision-Language-Action (VLA) system. Our goal is to build a simulated robot that can understand a spoken, high-level command and execute the corresponding physical task in a simulated environment.","sidebar":"roboticsTextbookSidebar"},"vla/llm-cognitive-planning":{"id":"vla/llm-cognitive-planning","title":"Cognitive Planning with LLMs","description":"Once a robot can understand spoken commands via a speech-to-text model like Whisper, the next challenge is to turn a high-level command like \"clean up the table\" into a concrete sequence of actions. This is the domain of task planning.","sidebar":"roboticsTextbookSidebar"},"vla/whisper-voice-commands":{"id":"vla/whisper-voice-commands","title":"Using Whisper for Voice Commands","description":"Welcome to our final chapter, where we bridge the gap between human language and robot action. Vision-Language-Action (VLA) models are at the forefront of AI, and the first step in building such a system is enabling the robot to understand spoken commands.","sidebar":"roboticsTextbookSidebar"}}}}